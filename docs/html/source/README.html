<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>zkynet: Exploring deep learning basics through implementations &mdash; zkynet  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="zkynet: exploring deep learning basics through implementations." href="../index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> zkynet
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">zkynet: Exploring deep learning basics through implementations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#concepts">Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#examples">Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#proof-of-concept-model">Proof-of-concept model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#composite-models-and-visualization">Composite models and visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#writing-an-operator">Writing an Operator</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#install-jax">Install JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optional-download-kaggle-datasets">(Optional): Download Kaggle Datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#progress-tracker">Progress tracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting-jax">Troubleshooting JAX</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">zkynet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>zkynet: Exploring deep learning basics through implementations</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/source/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="zkynet-exploring-deep-learning-basics-through-implementations">
<h1>zkynet: Exploring deep learning basics through implementations<a class="headerlink" href="#zkynet-exploring-deep-learning-basics-through-implementations" title="Permalink to this headline"></a></h1>
<p>An autodiff framework with a PyTorch-like API that supports tensor inputs and outputs.</p>
<p>Key points to note:</p>
<ul class="simple">
<li><p>all inputs to functions represented using our computational graph are
expected to be <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html">JAX arrays</a>.</p></li>
</ul>
<section id="concepts">
<h2>Concepts<a class="headerlink" href="#concepts" title="Permalink to this headline"></a></h2>
<p>The overall design principles of this framework is that:</p>
<ul>
<li><p>A <strong>function</strong> is an <em>abstract template</em> that maps ordered <strong>inputs</strong> (<code class="docutils literal notranslate"><span class="pre">Variable</span></code>)
to an output subject to some internal <strong>parameters</strong> (<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>));</p>
<p>In order to define the function, the values of these parameters
would need to be tracked of inside the Function object. For example,
think of a neural network model. The model carries parameters. But
before we apply it to some real inputs, the model exists as a floating
blackbox; it isn’t <em>grounded</em> to any input values.</p>
</li>
<li><p>There are two kinds of functions, operators (<code class="docutils literal notranslate"><span class="pre">Operator</span></code>) and
modules (<code class="docutils literal notranslate"><span class="pre">module</span></code>).</p>
<p>An <strong>operator</strong> is a function that we intend to hard code its
derivatives.  Such functions output numbers or arrays.</p>
<p>A <strong>module</strong> is a function that is intended to be
user-defined, (maybe) complicated functions.  There is a <em>flat</em>
<em>grounded</em> computational graph corresponding to a module,
created upon the module is called. This computational graph,
as explained next, consists of nodes that represent inputs
or operators (but not modules - that’s why we say it’s flat).</p>
</li>
<li><p>When concrete input values are provided for a function call, a
<strong>computational graph</strong> (a DAG) is created. We represent the
graph using nodes (<code class="docutils literal notranslate"><span class="pre">Node</span></code>). A <strong>node</strong> can always be
regarded as an instantiation of a particular Input to a
function. It carries a <strong>value</strong>.  Since it is a DAG, a node can
have multiple children and multiple parents.</p></li>
<li><p>We distinguish two node types: input node (<code class="docutils literal notranslate"><span class="pre">InputNode</span></code>) and
operator node (<code class="docutils literal notranslate"><span class="pre">OperatorNode</span></code>).</p>
<p>The <strong>input node</strong> is a leaf node. It literally represents a
leaf node on the DAG.</p>
<p>The <strong>operator node</strong> is not a leaf node.  Both
should be grounded with values.  The value of the operator
node represents the output of the function (specifically, an
Operator) under some input node instantiation.</p>
</li>
<li><p>A <strong>ModuleGraph</strong> is a computational graph that
is grounded when a Module is called. It stores
a flat computational graph (by ‘flat’ we mean
that its internal OperatorNodes should only be
Operators.)</p>
<p>Note that since a Module’s call may involve
calling another module, we don’t actually
create a graph for that module. We only care
about the trigger function (i.e. the first Module),
similar to CallSessionManager.</p>
</li>
<li><p>In order to enforce independence between computational
graphs from different calls, <strong>CallSessionManager</strong> will
maintain the <strong>call ID</strong> of the current call, which is assigned
to all nodes that are created during the call.</p>
<p>It will clear the call ID if the call to the trigger
function is finished (the trigger function is the first
function that is called, which is likely a user-defined
model).</p>
<p>Additionally, it stores InputNodes that have been
created (identified by its ID), so that subsequent
calls to the ‘to_node’ method of Input do not create
new ones (which may have wrong parent/children relationships)
but reuse the ones stored here.</p>
</li>
<li><p>Each Function or Input has a name and a functional name.</p>
<p><strong>Functional name:</strong> The name that identifies the ROLE this template
object plays in the definition of a function; For example,
if self is an Input, then this is the name that identifies
both the function and the role this input plays to that function.</p>
<p><strong>Name:</strong> The string that identifies the VARAIBLE name (or ENTITY)
that this template object represents. For example,
we could have two Function objects that represent
the same function but we care about their outputs
as separate variables. Then, these two Function objects
should have the same ‘functional_name’ but different
‘name’.</p>
</li>
</ul>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline"></a></h2>
<section id="proof-of-concept-model">
<h3>Proof-of-concept model<a class="headerlink" href="#proof-of-concept-model" title="Permalink to this headline"></a></h3>
<ol>
<li><p>Define a simple model for the function <code class="docutils literal notranslate"><span class="pre">f(x)</span> <span class="pre">=</span> <span class="pre">(x+w)*x^2</span></code> where <code class="docutils literal notranslate"><span class="pre">x</span></code> is an input and <code class="docutils literal notranslate"><span class="pre">w</span></code> is a parameter</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">zkynet.framework</span> <span class="kn">import</span> <span class="n">cg</span><span class="p">,</span> <span class="n">op</span>

<span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A rather simple function that represents:</span>

<span class="sd">    f(x,w) = (x+w)*x^2</span>

<span class="sd">    where x is an input and w is a parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w0</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),),</span>
                         <span class="n">params</span><span class="o">=</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">w0</span><span class="p">),))</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">))</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">mult</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
<p>Notice how the <code class="docutils literal notranslate"><span class="pre">call()</span></code> function defines the forward pass of
the model, where operations come from <code class="docutils literal notranslate"><span class="pre">zkynet.framework.op</span></code>
(short for <code class="docutils literal notranslate"><span class="pre">zkynet.framework.operations</span></code>). These are operations
specifically designed to work with our computational graph framework.</p>
<p><strong>Relation to PyTorch:</strong> (1) the <code class="docutils literal notranslate"><span class="pre">call()</span></code> function
is like <code class="docutils literal notranslate"><span class="pre">forward()</span></code> in a PyTorch nn.Module. (2) PyTorch
uses Tensors as its representation of values, which
have many built-in operations. We don’t rely on Tensor
(we are building from basic scratch) so we use our own
operators.</p>
</li>
<li><p>Forward pass:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">result</span><span class="o">.</span><span class="n">value</span>
<span class="c1"># 36</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">m(3)</span></code> calls the model and performs a forward pass,
with the input <code class="docutils literal notranslate"><span class="pre">x</span></code> set to value <code class="docutils literal notranslate"><span class="pre">3</span></code>. The output <code class="docutils literal notranslate"><span class="pre">result</span></code>
is of type <strong>ModuleGraph</strong> which represents a computational
graph that is grounded to the given input.</p>
<p>Note that each call produces an independent computational
graph. Namely:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">result1</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">result2</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">result1</span> <span class="o">!=</span> <span class="n">result2</span>
<span class="k">assert</span> <span class="n">result1</span><span class="o">.</span><span class="n">root</span> <span class="o">!=</span> <span class="n">result2</span><span class="o">.</span><span class="n">root</span>
</pre></div>
</div>
<p>Each call of the model generates a <em>call_id</em>, which is used
to distinguish between the computational graphs generated
for each call.</p>
<p>If you think of defining the model class as writing a
“template” of how inputs are associated to produce an
output, then a computational graph is an instantiation
of that template with all the placeholder inputs are filled
with concrete, given values.</p>
<p>Note that you could pass in a vector too:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
<span class="n">result</span><span class="o">.</span><span class="n">value</span>
<span class="c1"># array([ 36,  80, 150])</span>
</pre></div>
</div>
</li>
<li><p>Backprop (autodiff) &amp; gradients:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">result</span><span class="o">.</span><span class="n">back</span><span class="p">()</span>  <span class="c1"># backprop; accumulate gradients</span>
<span class="n">result</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">))</span>  <span class="c1"># obtain dm/dw</span>
<span class="c1"># 9</span>
<span class="n">result</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">))</span>  <span class="c1"># obtain dm/dx</span>
<span class="c1"># 33</span>
</pre></div>
</div>
<p>Note that backpropagation works with vector, matrix, or tensor inputs.
For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># vector input</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">result</span><span class="o">.</span><span class="n">back</span><span class="p">()</span>
<span class="n">result</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">))</span>
<span class="c1"># Out[7]:</span>
<span class="c1"># DeviceArray([[33.,  0.,  0.],</span>
<span class="c1">#              [ 0., 56.,  0.],</span>
<span class="c1">#              [ 0.,  0., 85.]], dtype=float32)</span>

<span class="n">result</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">))</span>
<span class="c1"># DeviceArray([[ 9.],</span>
<span class="c1">#              [16.],</span>
<span class="c1">#              [25.]], dtype=float32)</span>
</pre></div>
</div>
<p>See <a class="reference external" href="./tests/test_cg_backward.py"><code class="docutils literal notranslate"><span class="pre">test_cg_backward.py</span></code></a> for examples
of matrix and tensor inputs.</p>
</li>
</ol>
</section>
<section id="composite-models-and-visualization">
<h3>Composite models and visualization<a class="headerlink" href="#composite-models-and-visualization" title="Permalink to this headline"></a></h3>
<p>You can visualize a computational graph as
follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">zkynet.visual</span> <span class="kn">import</span> <span class="n">plot_cg</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_cg</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;simpel model&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This shows:</p>
<p><img alt="cg-simple" src="https://user-images.githubusercontent.com/7720184/167731761-bf651910-1a2a-463e-9384-41a4295c9f10.png" /></p>
<p>As a more useful example, we will visualize the computational graph
for a few models. First, let’s define a few more complex
models that are composed of the <code class="docutils literal notranslate"><span class="pre">SimpleModel</span></code>.
There are four cases in total:</p>
<p><strong>NO weight sharing, NO input sharing:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CompositeModel_NoWeightSharing_DifferentInputs</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;used to test composition&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">),</span>
                                 <span class="n">cg</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;x2&quot;</span><span class="p">)))</span>
        <span class="c1"># I expect the weights in the two may differ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_m1</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_m2</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_m1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_m2</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">CompositeModel_NoWeightSharing_DifferentInputs</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plot_cg</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;NoWeightSharing_DifferentInputs&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This generates:</p>
<p><img alt="cg-comp-NN" src="https://user-images.githubusercontent.com/7720184/167731793-d814fa88-3a23-44ae-92b5-6c7178718b47.png" /></p>
<p><strong>YES Weight sharing, NO input sharing:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CompositeModel_WeightSharing_DifferentInputs</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;used to test composition&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">),</span>
                                 <span class="n">cg</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;x2&quot;</span><span class="p">)))</span>
        <span class="c1"># I expect the weights in the two may differ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_m1</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">(</span><span class="n">w0</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_m1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_m1</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">CompositeModel_WeightSharing_DifferentInputs</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plot_cg</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;test_visualize_CompositeModel_WeightSharing_**Different**Inputs&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This generates:</p>
<p><img alt="cg-comp-YN" src="https://user-images.githubusercontent.com/7720184/167731827-8fe3555c-94f8-461c-92e1-48b4928eb64b.png" /></p>
<p><strong>NO weight sharing, YES sharing inputs:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CompositeModel_NoWeightSharing_SameInputs</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;used to test composition&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">),))</span>
        <span class="c1"># I expect the weights in the two may differ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_m1</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_m2</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_m1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_m2</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">CompositeModel_NoWeightSharing_SameInputs</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plot_cg</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;test_visualize_CompositeModel_**No**WeightSharing_**Same**Inputs&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This generates:</p>
<p><img alt="cg-comp-NY" src="https://user-images.githubusercontent.com/7720184/167731855-7520836f-89ca-4eb6-8104-50c109c51b9b.png" /></p>
<p><strong>YES weight sharing, YES sharing inputs:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CompositeModel_WeightSharing_SameInputs</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;used to test composition&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">cg</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">),))</span>
        <span class="c1"># I expect the weights in the two may differ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_m1</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_m1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_m1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">CompositeModel_WeightSharing_SameInputs</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plot_cg</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;test_visualize_CompositeModel_WeightSharing_**Same**Inputs&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This generates:</p>
<p><img alt="cg-comp-YY" src="https://user-images.githubusercontent.com/7720184/167731875-3d7f4476-e8a7-4037-b4a7-168dc336e77e.png" /></p>
</section>
<section id="writing-an-operator">
<h3>Writing an Operator<a class="headerlink" href="#writing-an-operator" title="Permalink to this headline"></a></h3>
<p>The interface is simple when implementing an operator (of class <code class="docutils literal notranslate"><span class="pre">Operator</span></code>). You only
need to implement the forward-pass in <code class="docutils literal notranslate"><span class="pre">_op_impl</span></code> (JAX takes care of the Jacobian
using <code class="docutils literal notranslate"><span class="pre">jacrev</span></code>; we only use <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> at the operator-level, and
implement <em>our own</em> automatic differentiation algorithm for module-level differentiation.)
Below are examples for <code class="docutils literal notranslate"><span class="pre">Add</span></code>, <code class="docutils literal notranslate"><span class="pre">Multiply</span></code>, <code class="docutils literal notranslate"><span class="pre">Square</span></code>, and <code class="docutils literal notranslate"><span class="pre">Dot</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">Operator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">),</span> <span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_op_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>


<span class="k">class</span> <span class="nc">Multiply</span><span class="p">(</span><span class="n">Operator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">),</span> <span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_op_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>  <span class="c1"># element wise multiplication</span>


<span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">Operator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),))</span>

    <span class="k">def</span> <span class="nf">_op_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Dot</span><span class="p">(</span><span class="n">Operator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">),</span> <span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_op_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that every time when using an operator to compose a function,
you need to create a new object for it. For convenience, you
could define utility functions like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Add</span><span class="p">()(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mult</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Multiply</span><span class="p">()(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Square</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Dot</span><span class="p">()(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline"></a></h2>
<p>Run <code class="docutils literal notranslate"><span class="pre">setup.sh</span></code> to create and activate a designated virtualenv.
The first time the virtualenv is activated, the script will install
dependency packages.</p>
<section id="install-jax">
<h3>Install JAX<a class="headerlink" href="#install-jax" title="Permalink to this headline"></a></h3>
<p>For some parts of the codebase, you may need to use JAX.</p>
<p>Install <a class="reference external" href="https://github.com/google/jax">JAX</a>.
Follow the steps for <a class="reference external" href="https://github.com/google/jax#pip-installation-gpu-cuda">GPU (CUDA)</a>.
Essentially:</p>
<ol>
<li><p>Make sure your CUDA and cuDNN versions are correct</p></li>
<li><p>Then run a pip install command to install <code class="docutils literal notranslate"><span class="pre">jax[cuda]</span></code>. As of 05/02/2022:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="s2">&quot;jax[cuda]&quot;</span> <span class="o">-</span><span class="n">f</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">storage</span><span class="o">.</span><span class="n">googleapis</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">jax</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="n">jax_releases</span><span class="o">.</span><span class="n">html</span>  <span class="c1"># Note: wheels only available on linux.</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="optional-download-kaggle-datasets">
<h3>(Optional): Download Kaggle Datasets<a class="headerlink" href="#optional-download-kaggle-datasets" title="Permalink to this headline"></a></h3>
<p>Run <code class="docutils literal notranslate"><span class="pre">download_datasets.sh</span></code></p>
<p>This will download several kaggle datasets.</p>
</section>
</section>
<section id="progress-tracker">
<h2>Progress tracker<a class="headerlink" href="#progress-tracker" title="Permalink to this headline"></a></h2>
<ol class="simple">
<li><p>Build a framework for automatic differentiation</p>
<ul class="simple">
<li><p>Design with a PyTorch-like API</p></li>
<li><p>Able to do forward pass</p></li>
<li><p>Able to do backward pass (backprop) for scalar input/output</p></li>
<li><p>Extension to vector, matrix, tensor input/output (using JAX)</p></li>
</ul>
</li>
</ol>
</section>
<section id="troubleshooting-jax">
<h2>Troubleshooting JAX<a class="headerlink" href="#troubleshooting-jax" title="Permalink to this headline"></a></h2>
<p>If you experience the following error messages when performing a seemingly simple
operation such as <code class="docutils literal notranslate"><span class="pre">jnp.dot</span></code>,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2022</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">14</span> <span class="mi">00</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">33.943054</span><span class="p">:</span> <span class="n">E</span> <span class="n">external</span><span class="o">/</span><span class="n">org_tensorflow</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">stream_executor</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="n">cuda_blas</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">232</span><span class="p">]</span> <span class="n">failed</span> <span class="n">to</span> <span class="n">create</span> <span class="n">cublas</span> <span class="n">handle</span><span class="p">:</span> <span class="n">CUBLAS_STATUS_NOT_INITIALIZED</span>
<span class="mi">2022</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">14</span> <span class="mi">00</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">33.943074</span><span class="p">:</span> <span class="n">E</span> <span class="n">external</span><span class="o">/</span><span class="n">org_tensorflow</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">stream_executor</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="n">cuda_blas</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">234</span><span class="p">]</span> <span class="n">Failure</span> <span class="n">to</span> <span class="n">initialize</span> <span class="n">cublas</span> <span class="n">may</span> <span class="n">be</span> <span class="n">due</span> <span class="n">to</span> <span class="n">OOM</span> <span class="p">(</span><span class="n">cublas</span> <span class="n">needs</span> <span class="n">some</span> <span class="n">free</span> <span class="n">memory</span> <span class="n">when</span> <span class="n">you</span>
 <span class="n">initialize</span> <span class="n">it</span><span class="p">,</span> <span class="ow">and</span> <span class="n">your</span> <span class="n">deep</span><span class="o">-</span><span class="n">learning</span> <span class="n">framework</span> <span class="n">may</span> <span class="n">have</span> <span class="n">preallocated</span> <span class="n">more</span> <span class="n">than</span> <span class="n">its</span> <span class="n">fair</span> <span class="n">share</span><span class="p">),</span> <span class="ow">or</span> <span class="n">may</span> <span class="n">be</span> <span class="n">because</span> <span class="n">this</span> <span class="n">binary</span> <span class="n">was</span> <span class="ow">not</span> <span class="n">built</span> <span class="k">with</span> <span class="n">support</span> <span class="k">for</span> <span class="n">the</span> <span class="n">GPU</span> <span class="ow">in</span> <span class="n">your</span> <span class="n">machine</span><span class="o">.</span>
<span class="mi">2022</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">14</span> <span class="mi">00</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">33.943104</span><span class="p">:</span> <span class="n">F</span> <span class="n">external</span><span class="o">/</span><span class="n">org_tensorflow</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">gemm_algorithm_picker</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">324</span><span class="p">]</span> <span class="n">Check</span> <span class="n">failed</span><span class="p">:</span> <span class="n">stream</span><span class="o">-&gt;</span><span class="n">parent</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">GetBlasGemmAlgorithms</span><span class="p">(</span><span class="o">&amp;</span><span class="n">algorithms</span><span class="p">)</span>
<span class="n">Aborted</span> <span class="p">(</span><span class="n">core</span> <span class="n">dumped</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, according to <a class="reference external" href="https://github.com/google/jax/issues/7118">this github thread</a>,
there maybe something funky with your CUDA / JAX version stuff. And you should run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">XLA_PYTHON_CLIENT_PREALLOCATE</span><span class="o">=</span><span class="n">false</span>
</pre></div>
</div>
<p>although I am not sure if with that GPU still gets used? <strong>YES. I CAN CONFIRM THAT IT IS USED; The memory is dynamically allocated.</strong></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="zkynet: exploring deep learning basics through implementations." accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Kaiyu Zheng.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>