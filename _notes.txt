

## APPENDIX: JAX


The JAX library provides a high-level interface
`jax.numpy` and a low-level interface `jax.lax`. The
low-level interface is stricter, but often more
powerful. Functions in `jax.numpy` eventually get
passed down to calls to `jax.lax` functions.

In JAX, arrays are ALWAYS immutable.

The main feature of JAX is "Just-In-Time" compilation,
which means code gets compiled the first time they
are run. Doc says:
>Not all JAX code can be JIT compiled, as it requires array shapes to be static & known at compile time

The way JAX can do JIT is because it expresses
its operations in terms of XLA (the Accelerated Linear
Algebra compiler).

In fact, people use JAX as a deep learning framework
too - alongside PyTorch. See [[this reddit post](https://www.reddit.com/r/MachineLearning/comments/shsfkm/d_current_state_of_jax_vs_pytorch/hv4h3k7/).

### autograd

[autograd](https://github.com/HIPS/autograd) is a library that differentiates native Python and Numpy code.
Pytorch either uses this library or implements something like it [torch.autograd](https://pytorch.org/docs/stable/autograd.html)
that works on Pytorch's tensors (instead of native python / numpy data structures).

Autograd's `grad` function takes in a function, and gives you a function that
computes its derivative.

### Thoughts

一开始我觉得Autograd很神奇，跟tensorflow不一样。但是，实际上他们是一样的。
背后都有一个operation framework，只不过tensorflow是用自定义的framework，
而autograd利用的numpy和python自带的，比较轻便罢了。本质上的原理没有区别。
